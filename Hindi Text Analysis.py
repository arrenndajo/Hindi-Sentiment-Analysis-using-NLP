# -*- coding: utf-8 -*-
"""Hindi Text Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sTilLhWMYwgzo_XKWyQh80aRDkMsnd4X

# Importing Libraries
"""

import pandas as pd
import numpy as np
pd.set_option('display.max_colwidth',None)   #this displays the dataframe in full width
import collections
from collections import Counter

!pip install clean-text
!pip install emoji

"""# Importing Dataset"""

df_sarcastic = pd.read_csv('/content/drive/MyDrive/SEM 7/NLP/Sarcasm_Hindi_Tweets-SARCASTIC.csv')
df_non_sarcastic = pd.read_csv('/content/drive/MyDrive/SEM 7/NLP/Sarcasm_Hindi_Tweets-NON-SARCASTIC.csv')
df_sarcastic['label'] = 'sarcastic'
df_non_sarcastic['label'] = 'non_sarcastic'
df = pd.concat([df_sarcastic, df_non_sarcastic], axis=0)
df = df.drop(['username','acctdesc','location','following','followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts', 'retweetcount', 'hashtags'] ,axis=1)
df = df.reset_index()
df = df.drop('index',axis=1)

def count_length():
    df['word_count'] = df['text'].apply(lambda x: len(str(x).split(" ")))

count_length()
df

"""# Removing Emoji from Text"""

import emoji

df = pd.DataFrame(df)
df['text'] = df['text'].apply(lambda s: emoji.replace_emoji(s, ' '))
df

import pandas as pd
import nltk
from nltk.util import ngrams
from collections import defaultdict
nltk.download("punkt")

"""# Sample Tokenization of 1 sentence"""

from nltk.tokenize import word_tokenize

corpus ="हमारे गांव में एक मास्टर जी रहते थे, वो मोहल्ले के बच्चों को अपने स्कूल आने को कहते थे। जब मैं बड़ा हुआ तो पता चला- स्कूल मास्टर के बेटे का, बस उनके भतीजे की, कैंटीन उनके भांजे की और पढ़ाने वाले भी सब उनके रिश्तेदार ही थे..! \n\nआज मास्टर जी में सरकार में मंत्री हैं.! \n\n#कटाक्ष"
tokens = word_tokenize(corpus)

print(tokens)

"""# Tokenization of the whole dataset"""

# Commented out IPython magic to ensure Python compatibility.
import nltk
nltk.download('stopwords')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import collections
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.cm as cm
import matplotlib.pyplot as plt
# %matplotlib inline
import nltk
from nltk.corpus import stopwords
import string
from sklearn.feature_extraction.text import CountVectorizer
import re

def message_cleaning(message):
    Test_punc_removed = [char for char in message if char not in string.punctuation]
    Test_punc_removed_join = ''.join(Test_punc_removed)
    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
    return Test_punc_removed_join_clean

df['text'] = df['text'].apply(message_cleaning)

count_length()
df

"""#Distribution of each Tweet-length per Label after cleaning data"""

corpus_list =[]
for i in range(len(df)):
    corpus_list +=df['text'][i]
counter=collections.Counter(corpus_list)
print(counter)

to_remove = ['नेहरू', 'लेते', 'कटाक्ष', 'जय', 'शी', 'अगर', 'मास्टर', 'वो', 'सिगरेट', 'बीवी', 'इश्क़', 'किताब', 'वश', 'पटाकर', 'पिलाकर']
for i in range(len(df)):
    df['text'][i]=[ele for ele in df['text'][i] if ele not in (to_remove)]
count_length()

least_common= [word for word, word_count in Counter(corpus_list).most_common()[:-50:-1]]
for i in range(len(df)):
    df['text'][i]=[ele for ele in df['text'][i] if ele not in (least_common)]

df

"""# Plotting Distribution of each Tweet-length per Label after cleaning data"""

df.hist(column = 'word_count', by = 'label',figsize=(12,4), bins = 5)

